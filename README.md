# MDEval

A benchmark to evaluate the _Markdown Awareness_ of LLMs chatbot's outputs.

## Human Evaluation Arena

To verify the effectiveness of the proposed benchmark, we built a human evaluation arena where we asked human annotators to evaluate the outputs of the LLMs chatbots. The arena is available at [this link](https://md-eval-human.pages.dev/).
